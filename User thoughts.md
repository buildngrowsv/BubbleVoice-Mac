What stack should we consider for the mac app? What if we want both a menu bar app and a desktop app? Or could we build it in a way that's platform agnostic like electron, consider the alternatives. Especially consider 2 additional things: UI and graphics and animation and overall UI quality and smoothness and ability to develop novel UI interactions, of course game engine would come closer here but it's not something you could excel at yourself. Instead, we should think about what approach would be best for you to be familiar with. The other thing is the ability to run voice models locally and indexes too. Primarily TTS but STT might be a focus too although apple's own option works fine. Say commands are great, could be interesting to build it with that in mind. Use Siri's voice? Assuming we don't care about getting on the app store



You still need to have access to premium models. So how do you monetize or give access. Enter your own API pretty much. Unless we find a cheap end to end option. I know several of the latest models are very cheap. Nearly too cheap to meter or something like that. It would need to be good at structured output and agentic implementations. Or we need to develop a benchmark for prompts and models with human in the loop as part of our development to optimize the system prompts and other prompts. The idea is for this agent to create UIs for user to consume. Should we consider HTML instead of structured UIs with bounds. Like I was thinking make a type of table and a type of diagram and a type of this and that but now I'm thinking why not have it do UI with HTML? I wanted to have it generate photos as the user speaks or as the AI speaks to depict things which is better but much more costly. So now I want to include both. Like why not? How hard could it be to include both? Could we generate and render html on mobile? How about on desktop?





You will need to ultrathink here. Read and interpret in a new doc. Use semantic searches as well. What I see:

AI voice and chat. Voice native chat second. Generates multiple assets potentially per conversation. Conversation have entries. It can go off to remind you of a call like a ringer and or menu bar.

When you are speaking, you will see bubbles. Prompts of things you could be talking about. These are afterthoughts of the agent that bubble up while the user is speaking or even as the agent is speaking. User can tap a bubble and it will save. Bubbles could be questions or just token limited outputs. Like idk 7 words max maybe makes sense.

Every prompt has a structured output component that the agent can fill out. Like a tool call. Filling out the afterthought prompts that will come up later in the call. Or they appear at the bottom as a row of bubbles that can be horizontally scrolled through. Or they float like bubbles. Idk. Or it's a toggle.

That's one region then when components get generated. Standalone html files. I think all of this can fit in idk how many tokens should we allocate and have that be our testing ground. Or scenarios we should build essentially of conversations and what kind of UI we expect to be generated. I want to push the limit of course. We have relatively long conversations at times and we also need to have a summarization step that captures important things to compress context. But you can't compress context as much with these artifacts you generate especially the html. Maybe you have a separate agent updating it. You need to also have a gracious failure state like a revert to working state. Then the user in the side panel can see the different components generated by the AI through the chat. And they can bring one into context by switching to it. On the right side there is a side panel. Or maybe we move it to the left side. List of chats on the left and a view window and the voice is on the right. The voice screen is bubbles and the text conversation back and forth. Transcribed and otherwise.

The issue is the degradation of the model with more context and more structured output. But if we collapse structured output to sophisticated potentially json prompt for image gen that could work, it would need to be on a good model that can generate text and graphics very accurately I think image 1 and nano banana pro and such.

Maybe unlike the other providers, we actually make it a text box that you literally speak into and you see the text appear. And you can edit it while it's appearing before it sends. But you can edit without the spoken input showing up where your cursor is. So you can edit what was just said while still speaking into the front of the input text. Maybe we can beautify it as well as part of the LLM prompt as a structured output when it is not too long. I mean I'm stuffing the prompt/context in a lot of these. Need to know how well it can perform so the discovery needs to be part of the development plan and process.

Then for the html generation idk. That's a little harder to justify. Unless it's seamless. The issue is that a halucination could wipe out the work. How could we limit the overhaul rate of the agent. We want the data to persist and be edited more often than not. That's why I was thinking go with something more structured. Also want to be able to call up these things and dynamically feed vector match chunks with each prompt without a tool call. Tool call expands the vector match chunk pull in chunk size and number of vector things. So a tool call probably initiates a spearate agent that has context of pre-set html templates and updates them or something like that. Not sure yet. We need to think through this. How can we get reliability and persistence while also letting the user say let's start over or redo it this way and the agent responding as appropriate. So we need to maybe run an experiment of how much the html output changes over multiple passes when it's not warranted and when it is warranted and when the user wants to do the whole thing. Or is there an xml type modificaiton thing we could do that it edits certain lines but we still need to eval that as well.







Now we want to think about how to evaluate the right hierarchy for logging life areas. We need it probably in the form of a folder structure and documents where we capture information about the thing. A folder can be added to turn an item at the end into a subproject. So we need to allow the agent to make folder heriarchy changes where necessary. It needs to be able to append to a long document for that area. Instead of it being a maintenance step, we need it to be a pathway that the agnet can follow to check existing areas or create or edit or add to.

We would need to add a section in the prompt that sends the areas tree. We need some kind of organizing framework. So then the agent when responding can have structured output responses that add to or create a new note or create an area or read from it in the next step.

Let's keep working with gemini flash lite for now and we can bring other models for testing after we refine the overall architecture. These areas and their documents are essentially what will be vector searched from. There will probably be a standard folder level document or documents and then sub-documents where are endpoints. An endpiont could be a task or a task list. Or a standard for dated and timed events that are added to the main document for an area or another area. It will be appended at the top essentially. So newest thing stays at the top not the bottom. but below a summary section maybe.