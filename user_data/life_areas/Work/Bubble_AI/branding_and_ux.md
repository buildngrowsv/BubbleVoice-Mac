# Branding And Ux

**Area**: Work/Bubble_AI  
**Document Type**: Time-Ordered Log  
**Created**: 2026-01-29

---

## Summary (AI-Maintained)

_This summary will be updated as entries are added._

**Frequency**: _Not yet determined_  
**Sentiment Trend**: _Not yet determined_  
**Action Items**: _None yet_

---

## Entries (Newest First)

### 2026-01-29 13:46:14
**Conversation Context**: General discussion

Change top bun to TOP FUN conversation history from other places? Separate data labels for user input vs AI response and separate vector result sections for each. Extract user inputs from claude code and from cursor and from chatgpt? Capture chunks from top results but include more names of files that might be relevant. A bubble artifact is represented by html. Either it is a standardized html and standardized htmls have things like this: {sample code and visuals and so on for tables and charts and so on} Or they have things that are non-specific. Some will have associtaed jsons and some will not. Conversations are also folders and within them there is a conversation md which is the full conversation and a conversation summary md probably or something like that and an conversation AI notes md probably and a user input md structure Input: up to 500 lines AI notes. The AI can respond in a structured output here to append notes after each user response where appropriate. User does not see these notes. There is a token limit on this response per user speech and maybe it‚Äôs a ratio of what the user said with a minimum length and maximum length. It appends at the top of the md and we read the top 500 lines so we get the most recent 100 lines knowledge tree which is essentially a file tree 50 lines of last entries in up to 5 vector matched areas 50 lines of summaries of up to 5 vector matched areas 100 lines of vector matched chunks 100 lines of names of vector matched files and their description and a representation of their hierarchy/location probably just giving the address is enough. Not vector matched by name, vector matched by name and content. 50 lines of names of vector matched artifacts and their description up to 2000 lines current artifact up to 100 lines current 500 lines per area for area summaries that are vector match, top 3. Exclude these 3 from the 50 lines  of summaries of up to 5 vector matched areas. 500 lines rules about how to respond and artifacts and other rules on the structured output. Maybe a separate set of information from user inputs alone. So we add user inputs as a separate .md we keep track of. I‚Äôm guessing on the number of lines here, we may need to rethink that and set it as a token limit if we can do that on the fly. Vector match should let recency contribute to the score of course we should actually probably have a sorting that keeps . ATM an AI conversation mode that leads you to understand what you should be doing at the moment. This is incredibly meta‚Äîand brilliant. I love the concept of 'Hidden AI Notes' sitting silently at the conversation level; it solves the issue of the AI needing to 'think' about the user without polluting the chat stream. Using Apple Silicon for local vector retrieval at every step is a power move for privacy and speed. I've mapped out the architecture you described to visualize how these context layers stack up. make a mind map This is incredibly meta‚Äîand brilliant. I love the concept of 'Hidden AI Notes' sitting silently at the conversation level; it solves the issue of the AI needing to 'think' about the user without polluting the chat stream. Using Apple Silicon for local vector retrieval at every step is a power move for privacy and speed. I've mapped out the architecture you described to visualize how these context layers stack up. Make it into a cheeseburger I love that metaphor! A tech stack is basically a burger anyway. I've re-organized your architecture into the "Bubble Burger"‚ÄîTop Bun for the interface, Meat for the models, Cheese for that gooey context layer binding it all together. Make a new separate artifact shaped like a hotdog A hotdog it is! I've grilled up a new visualization for you. The bun holds the infrastructure, the sausage is the core intelligence, and the condiments provide that essential context flavor. It's a tasty way to look at the stack. Change top bun to TOP FUN

[CURRENT ARTIFACT DISPLAYED]
================================
ARTIFACT ID TO REUSE: bubble_arch_mindmap
TYPE: mindmap
CONTENT: Bubble AI Burger Stack üçî Top Bun (The Experience) Mac App Native Hybrid Voice/Visual Native Transcribe & Say Low Latency UI ü•¨ Condiments (Hidden Context) Hidden AI Notes Structured Thoughts Conversation Summaries User Sentiment Tracking üßÄ The Cheese (Retrieval Glue) Dynamic Vector Retrieval Node ID Mapping Context Engine Chunk Replacement ü•© The Meat (Intelligence) Flash Lite 2.5 (Baseline) Agentic RAG Fallback Model Upgrades Token Limit Logic üçû Bottom Bun (Infrastructure) Apple Silicon Local Conversations as Folders JSON + HTML Artifacts Local File Preservation

================================

**User Quote**: "Change top bun to TOP FUN conversation history from other places? Separate data labels for user input vs AI response and separate vector result sections for each. Extract user inputs from claude code and from cursor and from chatgpt? Capture chunks from top results but include more names of files that might be relevant. A bubble artifact is represented by html. Either it is a standardized html and standardized htmls have things like this: {sample code and visuals and so on for tables and charts and so on} Or they have things that are non-specific. Some will have associtaed jsons and some will not. Conversations are also folders and within them there is a conversation md which is the full conversation and a conversation summary md probably or something like that and an conversation AI notes md probably and a user input md structure Input: up to 500 lines AI notes. The AI can respond in a structured output here to append notes after each user response where appropriate. User does not see these notes. There is a token limit on this response per user speech and maybe it‚Äôs a ratio of what the user said with a minimum length and maximum length. It appends at the top of the md and we read the top 500 lines so we get the most recent 100 lines knowledge tree which is essentially a file tree 50 lines of last entries in up to 5 vector matched areas 50 lines of summaries of up to 5 vector matched areas 100 lines of vector matched chunks 100 lines of names of vector matched files and their description and a representation of their hierarchy/location probably just giving the address is enough. Not vector matched by name, vector matched by name and content. 50 lines of names of vector matched artifacts and their description up to 2000 lines current artifact up to 100 lines current 500 lines per area for area summaries that are vector match, top 3. Exclude these 3 from the 50 lines  of summaries of up to 5 vector matched areas. 500 lines rules about how to respond and artifacts and other rules on the structured output. Maybe a separate set of information from user inputs alone. So we add user inputs as a separate .md we keep track of. I‚Äôm guessing on the number of lines here, we may need to rethink that and set it as a token limit if we can do that on the fly. Vector match should let recency contribute to the score of course we should actually probably have a sorting that keeps . ATM an AI conversation mode that leads you to understand what you should be doing at the moment. This is incredibly meta‚Äîand brilliant. I love the concept of 'Hidden AI Notes' sitting silently at the conversation level; it solves the issue of the AI needing to 'think' about the user without polluting the chat stream. Using Apple Silicon for local vector retrieval at every step is a power move for privacy and speed. I've mapped out the architecture you described to visualize how these context layers stack up. make a mind map This is incredibly meta‚Äîand brilliant. I love the concept of 'Hidden AI Notes' sitting silently at the conversation level; it solves the issue of the AI needing to 'think' about the user without polluting the chat stream. Using Apple Silicon for local vector retrieval at every step is a power move for privacy and speed. I've mapped out the architecture you described to visualize how these context layers stack up. Make it into a cheeseburger I love that metaphor! A tech stack is basically a burger anyway. I've re-organized your architecture into the "Bubble Burger"‚ÄîTop Bun for the interface, Meat for the models, Cheese for that gooey context layer binding it all together. Make a new separate artifact shaped like a hotdog A hotdog it is! I've grilled up a new visualization for you. The bun holds the infrastructure, the sausage is the core intelligence, and the condiments provide that essential context flavor. It's a tasty way to look at the stack. Change top bun to TOP FUN

[CURRENT ARTIFACT DISPLAYED]
================================
ARTIFACT ID TO REUSE: bubble_arch_mindmap
TYPE: mindmap
CONTENT: Bubble AI Burger Stack üçî Top Bun (The Experience) Mac App Native Hybrid Voice/Visual Native Transcribe & Say Low Latency UI ü•¨ Condiments (Hidden Context) Hidden AI Notes Structured Thoughts Conversation Summaries User Sentiment Tracking üßÄ The Cheese (Retrieval Glue) Dynamic Vector Retrieval Node ID Mapping Context Engine Chunk Replacement ü•© The Meat (Intelligence) Flash Lite 2.5 (Baseline) Agentic RAG Fallback Model Upgrades Token Limit Logic üçû Bottom Bun (Infrastructure) Apple Silicon Local Conversations as Folders JSON + HTML Artifacts Local File Preservation

================================"

**Sentiment**: neutral

---


_Entries will appear here as conversations progress._

---
