/// VPIOEchoCancellationTest.swift
///
/// PURPOSE: Tests whether Apple's Voice Processing IO (VPIO) provides echo cancellation
/// on macOS when TTS audio is played through speakers while the microphone is active.
///
/// ARCHITECTURE:
/// - AVAudioEngine with setVoiceProcessingEnabled(true) on outputNode
/// - AVAudioPlayerNode plays TTS audio (generated by macOS `say` command)
/// - inputNode captures microphone audio with VPIO's AEC applied
/// - SpeechAnalyzer (macOS 26+) transcribes the AEC-cleaned mic signal
///
/// WHY THIS MATTERS:
/// In the Accountability project's code, the ACTUAL echo cancellation comes from VPIO,
/// not from WebRTC. WebRTC's dual-peer system is used for iOS audio classification
/// (making iOS treat audio as "call audio" so it's not suppressed). On macOS, we may
/// not need the WebRTC dual-peer system at all if VPIO alone provides AEC.
///
/// EXPECTED RESULTS:
/// - If VPIO AEC works: TTS words should NOT appear in transcription (echo cancelled)
/// - If VPIO AEC fails: TTS words WILL appear in transcription (echo detected)
///
/// HOW TO RUN:
/// swiftc -parse-as-library VPIOEchoCancellationTest.swift -framework AVFoundation -framework Speech -o vpio_test && ./vpio_test

import AVFoundation
import Foundation
import Speech

/// setStdoutUnbuffered - ensures immediate console output in command-line Swift tools
/// Without this, stdout is line-buffered and print statements may not appear until much later
func setStdoutUnbuffered() {
    setbuf(stdout, nil)
}

/// VPIOEchoCancellationTest - Main test class that manages the entire echo cancellation test
///
/// The test runs in three phases:
/// 1. Baseline: Listen for 5 seconds WITHOUT TTS to verify transcription works
/// 2. TTS Playback: Play TTS through speakers while continuing to transcribe
/// 3. Post-TTS: Listen for 5 more seconds after TTS ends
///
/// The test then compares transcribed words against TTS words to detect echo.
///
/// DESIGN DECISION: We use a class with instance variables (not local variables in async
/// closures) because SpeechAnalyzer and its continuation must remain alive for the entire
/// test duration. Local variables in async contexts can be prematurely deallocated,
/// causing zero transcription results. This was discovered during earlier sandbox testing.
@available(macOS 26.0, *)
class VPIOEchoCancellationTest {
    // MARK: - Audio Engine Components
    
    /// The core audio engine - manages all audio routing and processing
    /// When VPIO is enabled on its outputNode, it provides hardware-level AEC
    var audioEngine = AVAudioEngine()
    
    /// Player node for TTS audio playback
    /// TTS files generated by `say` command are scheduled on this node
    /// Audio flows: playerNode → mainMixerNode → outputNode → speakers
    let playerNode = AVAudioPlayerNode()
    
    // MARK: - Speech Recognition Components
    
    /// SpeechAnalyzer instance - the macOS 26+ speech recognition API
    /// Must be kept alive as an instance variable to prevent premature deallocation
    var analyzer: SpeechAnalyzer?
    
    /// SpeechTranscriber - produces transcription results from the analyzer
    var transcriber: SpeechTranscriber?
    
    /// Continuation for feeding audio buffers to SpeechAnalyzer
    /// Must be kept alive as instance variable (see class-level comment about lifecycle)
    var continuation: AsyncStream<AnalyzerInput>.Continuation?
    
    /// Task that consumes transcription results asynchronously
    var resultTask: Task<Void, Error>?
    
    // MARK: - Test State
    
    /// All transcribed text segments, captured during the test
    /// Each entry is a progressive transcription (SpeechAnalyzer provides volatile/partial results)
    var transcribedTexts: [String] = []
    
    /// Transcriptions captured DURING TTS playback (Phase 2)
    /// These are the ones we check for echo - if TTS words appear here, AEC failed
    var duringTTSTexts: [String] = []
    
    /// Transcriptions captured BEFORE TTS playback (Phase 1 - baseline)
    var beforeTTSTexts: [String] = []
    
    /// Transcriptions captured AFTER TTS playback (Phase 3)
    var afterTTSTexts: [String] = []
    
    /// The TTS text that will be played through speakers
    /// We check transcriptions against these words to detect echo
    let ttsText = "The quick brown fox jumps over the lazy sleeping dog in the sunny meadow"
    
    /// Running frame count for CMTime calculation
    /// SpeechAnalyzer needs monotonically increasing timestamps for each audio buffer
    var frameCount: Int64 = 0
    
    /// Flag to track which phase we're in for categorizing transcriptions
    var currentPhase = "before"
    
    // MARK: - Main Test Entry Point
    
    func run() async throws {
        print("============================================")
        print("  VPIO Echo Cancellation Test")
        print("  Tests if Apple's Voice Processing IO")
        print("  cancels TTS echo on macOS")
        print("============================================")
        print("")
        
        // STEP 1: Enable VPIO on the audio engine
        // This is the KEY step - VPIO provides AEC at the hardware/OS level
        // When enabled, the outputNode and inputNode share a Voice Processing IO audio unit
        // The VPIO unit knows what audio is going to the speakers and subtracts it from the mic input
        print("[STEP 1] Enabling Voice Processing IO (VPIO)...")
        do {
            try audioEngine.outputNode.setVoiceProcessingEnabled(true)
            print("  VPIO enabled: \(audioEngine.outputNode.isVoiceProcessingEnabled)")
        } catch {
            print("  FAILED to enable VPIO: \(error)")
            print("  This means AEC will NOT work - echo is expected")
        }
        
        // STEP 2: Generate TTS audio file using macOS `say` command
        // We use `say` because it provides high-quality Siri voices
        // The audio is saved to a file, then played through AVAudioPlayerNode
        print("")
        print("[STEP 2] Generating TTS audio with `say` command...")
        let ttsPath = "/tmp/vpio_echo_test_tts.aiff"
        let ttsURL = URL(fileURLWithPath: ttsPath)
        
        // Clean up any previous test file
        try? FileManager.default.removeItem(at: ttsURL)
        
        let sayProcess = Process()
        sayProcess.executableURL = URL(fileURLWithPath: "/usr/bin/say")
        sayProcess.arguments = ["-o", ttsPath, ttsText]
        try sayProcess.run()
        sayProcess.waitUntilExit()
        
        guard FileManager.default.fileExists(atPath: ttsPath) else {
            print("  FAILED: TTS file was not generated")
            return
        }
        
        let audioFile = try AVAudioFile(forReading: ttsURL)
        let ttsDuration = Double(audioFile.length) / audioFile.fileFormat.sampleRate
        print("  TTS text: \"\(ttsText)\"")
        print("  TTS duration: \(String(format: "%.2f", ttsDuration))s")
        print("  TTS format: \(audioFile.processingFormat)")
        
        // STEP 3: Set up audio engine graph
        // When VPIO is enabled, the audio engine's nodes operate through a Voice Processing
        // IO audio unit that has specific format requirements. We must:
        // 1. Explicitly connect mainMixerNode → outputNode with the output format
        // 2. Attach the player node and connect to mixer
        //
        // This mirrors Accountability's AVAudioEngineRTCAudioDevice which does:
        //   audioEngine.connect(audioEngine.mainMixerNode, to: audioEngine.outputNode, format: outputFormat)
        // before starting the engine.
        print("")
        print("[STEP 3] Setting up audio engine graph...")
        
        // Query the engine's native format AFTER VPIO is enabled
        // VPIO changes the audio unit configuration to an aggregate device
        let outputFormat = audioEngine.outputNode.outputFormat(forBus: 0)
        print("  Output node format: \(outputFormat)")
        
        // CRITICAL: Explicitly connect mixer → output with the proper VPIO format
        // Without this, the engine fails to initialize with error -10875
        audioEngine.connect(audioEngine.mainMixerNode, to: audioEngine.outputNode, format: outputFormat)
        print("  Mixer → Output connected with format: \(outputFormat)")
        
        // Attach and connect player node
        audioEngine.attach(playerNode)
        // Use format: nil to let AVAudioEngine auto-convert from the file's format
        // to the engine's native format. This is critical when VPIO is active.
        audioEngine.connect(playerNode, to: audioEngine.mainMixerNode, format: nil)
        print("  Player node attached and connected to mixer (auto-format conversion)")
        
        // STEP 4: Set up SpeechAnalyzer for transcription
        // We use SpeechAnalyzer (macOS 26+) instead of SFSpeechRecognizer because
        // it provides better on-device recognition and is the modern API
        print("")
        print("[STEP 4] Setting up SpeechAnalyzer...")
        
        guard let locale = await SpeechTranscriber.supportedLocale(equivalentTo: Locale(identifier: "en-US")) else {
            print("  FAILED: en-US locale not supported")
            return
        }
        
        let transcriber = SpeechTranscriber(
            locale: locale,
            transcriptionOptions: [],
            reportingOptions: [.volatileResults],
            attributeOptions: []
        )
        self.transcriber = transcriber
        
        guard let analyzerFormat = await SpeechAnalyzer.bestAvailableAudioFormat(compatibleWith: [transcriber]) else {
            print("  FAILED: No compatible audio format for SpeechAnalyzer")
            return
        }
        
        print("  Analyzer format: \(analyzerFormat)")
        
        let analyzer = SpeechAnalyzer(modules: [transcriber])
        self.analyzer = analyzer
        
        // Create the async stream that feeds audio buffers to SpeechAnalyzer
        let (stream, continuation) = AsyncStream<AnalyzerInput>.makeStream()
        self.continuation = continuation
        
        // STEP 5: Start result consumption task
        // This task runs continuously, collecting transcription results as they arrive
        print("")
        print("[STEP 5] Starting result consumer...")
        resultTask = Task {
            for try await result in transcriber.results {
                let text = String(result.text.characters)
                if !text.isEmpty {
                    print("  TRANSCRIPT [\(self.currentPhase)]: \"\(text)\"")
                    fflush(stdout)
                    self.transcribedTexts.append(text)
                    
                    // Categorize by phase
                    switch self.currentPhase {
                    case "before":
                        self.beforeTTSTexts.append(text)
                    case "during":
                        self.duringTTSTexts.append(text)
                    case "after":
                        self.afterTTSTexts.append(text)
                    default:
                        break
                    }
                }
            }
        }
        
        // STEP 6: Install audio tap on inputNode to feed SpeechAnalyzer
        // The inputNode captures microphone audio. With VPIO enabled, this audio
        // has AEC applied - meaning the TTS audio playing through speakers should
        // be subtracted from the microphone signal.
        //
        // IMPORTANT: When VPIO is enabled, inputNode may report a multichannel format
        // (e.g., 9 channels) because it's an aggregate VPIO device. We install the tap
        // with format: nil to get whatever format the engine provides, then convert
        // to SpeechAnalyzer's expected mono 16kHz Int16 format.
        print("")
        print("[STEP 6] Installing audio tap on inputNode...")
        let inputNode = audioEngine.inputNode
        let inputFormat = inputNode.outputFormat(forBus: 0)
        print("  Input format (raw): \(inputFormat)")
        
        // Create a simpler mono float32 format at the input's sample rate for the tap
        // This avoids issues with the multichannel VPIO aggregate format
        let tapFormat = AVAudioFormat(
            commonFormat: .pcmFormatFloat32,
            sampleRate: inputFormat.sampleRate,
            channels: 1,
            interleaved: false
        )!
        print("  Tap format (mono): \(tapFormat)")
        
        guard let converter = AVAudioConverter(from: tapFormat, to: analyzerFormat) else {
            print("  FAILED: Could not create audio converter")
            return
        }
        
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: tapFormat) { [weak self] buffer, time in
            guard let self = self else { return }
            
            // Convert from tap format (mono float32 48kHz) to SpeechAnalyzer format (mono int16 16kHz)
            let ratio = analyzerFormat.sampleRate / tapFormat.sampleRate
            let outputFrameCapacity = AVAudioFrameCount(Double(buffer.frameLength) * ratio)
            guard outputFrameCapacity > 0,
                  let convertedBuffer = AVAudioPCMBuffer(
                pcmFormat: analyzerFormat,
                frameCapacity: outputFrameCapacity
            ) else { return }
            
            var error: NSError?
            converter.convert(to: convertedBuffer, error: &error) { _, outStatus in
                outStatus.pointee = .haveData
                return buffer
            }
            
            if error == nil && convertedBuffer.frameLength > 0 {
                // Create monotonically increasing timestamp for SpeechAnalyzer
                let cmTime = CMTime(value: CMTimeValue(self.frameCount), timescale: CMTimeScale(analyzerFormat.sampleRate))
                self.frameCount += Int64(convertedBuffer.frameLength)
                self.continuation?.yield(AnalyzerInput(buffer: convertedBuffer, bufferStartTime: cmTime))
            }
        }
        
        // STEP 7: Start the audio engine and SpeechAnalyzer
        print("")
        print("[STEP 7] Starting audio engine and analyzer...")
        audioEngine.prepare()
        try audioEngine.start()
        print("  Audio engine started")
        print("  Engine running: \(audioEngine.isRunning)")
        print("  VPIO still enabled: \(audioEngine.outputNode.isVoiceProcessingEnabled)")
        
        try await analyzer.start(inputSequence: stream)
        print("  SpeechAnalyzer started")
        
        // PHASE 1: Baseline - listen without TTS
        // This verifies that SpeechAnalyzer is working and can transcribe background speech
        print("")
        print("============================================")
        print("  PHASE 1: Baseline (5 seconds)")
        print("  Listening WITHOUT TTS playback")
        print("  Speak or play audio from another device")
        print("  to verify transcription is working")
        print("============================================")
        currentPhase = "before"
        try await Task.sleep(for: .seconds(5))
        
        // PHASE 2: TTS Playback - the critical echo cancellation test
        // We play TTS through speakers and check if it leaks into transcription
        print("")
        print("============================================")
        print("  PHASE 2: TTS Playing (\(String(format: "%.1f", ttsDuration))s + buffer)")
        print("  TTS: \"\(ttsText)\"")
        print("  If AEC works: TTS should NOT appear in transcription")
        print("  If AEC fails: TTS WILL appear in transcription")
        print("============================================")
        currentPhase = "during"
        
        // Schedule and play the TTS audio
        playerNode.scheduleFile(audioFile, at: nil) {
            print("  >> TTS playback completed")
            fflush(stdout)
        }
        playerNode.play()
        print("  >> TTS started playing")
        
        // Wait for TTS duration + 3 second buffer for lingering transcriptions
        try await Task.sleep(for: .seconds(ttsDuration + 3.0))
        
        // PHASE 3: Post-TTS - listen after TTS stops
        print("")
        print("============================================")
        print("  PHASE 3: Post-TTS (5 seconds)")
        print("  TTS has stopped, continuing to listen")
        print("============================================")
        currentPhase = "after"
        try await Task.sleep(for: .seconds(5))
        
        // STEP 8: Finalize and analyze results
        print("")
        print("[STEP 8] Finalizing analyzer...")
        try await analyzer.finalize(through: nil)
        
        // Give a moment for any final results
        try await Task.sleep(for: .seconds(1))
        
        // MARK: - Results Analysis
        print("")
        print("============================================")
        print("  RESULTS")
        print("============================================")
        print("")
        
        print("TTS Text: \"\(ttsText)\"")
        print("")
        
        print("Phase 1 (Before TTS): \(beforeTTSTexts.count) transcriptions")
        for text in beforeTTSTexts {
            print("  - \"\(text)\"")
        }
        print("")
        
        print("Phase 2 (During TTS): \(duringTTSTexts.count) transcriptions")
        for text in duringTTSTexts {
            print("  - \"\(text)\"")
        }
        print("")
        
        print("Phase 3 (After TTS): \(afterTTSTexts.count) transcriptions")
        for text in afterTTSTexts {
            print("  - \"\(text)\"")
        }
        print("")
        
        // Echo detection: check if DISTINCTIVE TTS words appear in Phase 2 transcriptions
        //
        // IMPORTANT: We exclude common English stop words (the, a, in, over, etc.) because
        // these appear naturally in any English speech and would cause false positives.
        // We only check for distinctive/unique words from the TTS text.
        let commonStopWords: Set<String> = [
            "the", "a", "an", "in", "on", "at", "to", "for", "of", "is", "it",
            "and", "or", "but", "not", "no", "so", "do", "if", "by", "up", "as",
            "be", "he", "she", "we", "my", "me", "you", "his", "her", "our",
            "its", "was", "are", "has", "had", "been", "have", "that", "this",
            "with", "from", "they", "them", "will", "can", "over"
        ]
        
        // Extract distinctive TTS words (not stop words, length > 3)
        let allTTSWords = ttsText.lowercased().components(separatedBy: " ")
        let distinctiveTTSWords = Set(allTTSWords.filter { $0.count > 3 && !commonStopWords.contains($0) })
        
        print("Distinctive TTS words to check: \(distinctiveTTSWords.sorted().joined(separator: ", "))")
        print("")
        
        var echoWordCount = 0
        var echoWords: [String] = []
        
        // Only check the LAST (most complete) transcription from each utterance group
        // Volatile results give progressive transcriptions; the last one is most accurate
        let finalDuringTexts = duringTTSTexts
        
        for text in finalDuringTexts {
            let transcribedWords = Set(text.lowercased().components(separatedBy: " "))
            let matches = distinctiveTTSWords.intersection(transcribedWords)
            for word in matches {
                echoWordCount += 1
                if !echoWords.contains(word) {
                    echoWords.append(word)
                }
            }
        }
        
        print("============================================")
        print("  ECHO ANALYSIS")
        print("============================================")
        print("")
        print("Distinctive TTS words found in Phase 2 transcriptions: \(echoWordCount)")
        if !echoWords.isEmpty {
            print("Echo words: \(echoWords.joined(separator: ", "))")
        }
        print("")
        
        if echoWordCount == 0 && duringTTSTexts.isEmpty {
            print("RESULT: NO TRANSCRIPTIONS during TTS")
            print("  This could mean:")
            print("  - AEC is working (TTS echo cancelled, no other speech)")
            print("  - No background audio was present")
            print("  - Mic sensitivity is too low")
            print("  VERDICT: Likely AEC is working (need background speech to confirm)")
        } else if echoWordCount >= 3 {
            print("RESULT: ECHO DETECTED - \(echoWordCount) distinctive TTS words in transcription")
            print("  VPIO AEC is NOT effectively cancelling the TTS echo")
            print("  WebRTC routing may be needed for proper AEC")
        } else if echoWordCount > 0 {
            print("RESULT: PARTIAL ECHO - \(echoWordCount) distinctive TTS words (minor leakage)")
            print("  VPIO AEC is partially working but some leakage detected")
        } else {
            print("RESULT: NO ECHO DETECTED - AEC IS WORKING!")
            print("  None of the distinctive TTS words appeared in transcription")
            print("  Background audio was transcribed without TTS contamination")
            print("  VPIO (Voice Processing IO) provides effective AEC on macOS!")
        }
        
        // Cleanup
        print("")
        print("Cleaning up...")
        audioEngine.stop()
        inputNode.removeTap(onBus: 0)
        self.continuation?.finish()
        resultTask?.cancel()
        try? FileManager.default.removeItem(at: ttsURL)
        
        print("Test complete.")
    }
}

// MARK: - Entry Point

/// VPIOTestEntryPoint - wraps the test execution in a proper @main struct
/// Using @main avoids "expressions not allowed at top level" when compiling with -parse-as-library
@main
struct VPIOTestEntryPoint {
    static func main() {
        setStdoutUnbuffered()
        
        if #available(macOS 26.0, *) {
            Task {
                let test = VPIOEchoCancellationTest()
                do {
                    try await test.run()
                } catch {
                    print("Test failed with error: \(error)")
                }
                exit(0)
            }
            RunLoop.main.run()
        } else {
            print("Requires macOS 26+")
            exit(1)
        }
    }
}
