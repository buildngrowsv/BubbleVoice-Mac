/// TurnDetectionWithVPIOTest.swift
///
/// COMPLETE turn detection, caching, and interruption test with VPIO AEC.
/// Implements EVERY path from instructions.mdc:
///
///   PATH A (Normal):       silence 2s â†’ generate TTS â†’ play â†’ finish â†’ next turn
///   PATH B (Cache Hit):    silence 2s â†’ generating â†’ user 2+ words â†’ cancel & cache
///                          â†’ user stops 2s â†’ cache < 5s â†’ speak cached â†’ finish
///   PATH C (Cache Expiry): silence 2s â†’ generating â†’ user 2+ words â†’ cancel & cache
///                          â†’ user keeps talking > 5s â†’ cache expired â†’ discard
///                          â†’ next silence 2s â†’ fresh response
///   PATH D (Playback Int): playing TTS â†’ user 2+ words â†’ stop playback â†’ next turn
///
/// Instead of LLM, we echo back "Turn N: [transcription]" via `say`.
/// Runs for 5 completed turns (turns where TTS finishes or is interrupted during playback).
/// Maintains a conversation thread across all turns.
///
/// HOW TO RUN: ./run-turn-detection-test.sh
/// REQUIRES: macOS 26+, microphone permission, background audio from another device.

import AVFoundation
import Foundation
import Speech

func setStdoutUnbuffered() { setbuf(stdout, nil) }

// MARK: - File Logging

/// Global file handle for logging detailed test output
var logFileHandle: FileHandle?
var logFilePath: String?

func initializeLogFile() {
    let timestamp = DateFormatter().then {
        $0.dateFormat = "yyyyMMdd_HHmmss"
    }.string(from: Date())
    logFilePath = "/tmp/turn-detection-test-\(timestamp).log"
    
    FileManager.default.createFile(atPath: logFilePath!, contents: nil)
    logFileHandle = FileHandle(forWritingAtPath: logFilePath!)
    
    let header = """
    ================================================================
    TURN DETECTION WITH VPIO TEST - DETAILED LOG
    ================================================================
    Started: \(Date())
    Test: TurnDetectionWithVPIOTest.swift
    Purpose: Compare SpeechAnalyzer word-by-word output with timing
    ================================================================
    
    """
    writeToLog(header)
    print("ðŸ“ Logging to: \(logFilePath!)")
}

func writeToLog(_ message: String) {
    guard let handle = logFileHandle,
          let data = (message + "\n").data(using: .utf8) else { return }
    handle.write(data)
}

func closeLogFile() {
    logFileHandle?.closeFile()
    if let path = logFilePath {
        print("\nðŸ“ Log saved to: \(path)")
    }
}

extension DateFormatter {
    func then(_ block: (DateFormatter) -> Void) -> DateFormatter {
        block(self)
        return self
    }
}

// MARK: - State Machine

/// Every possible state the agent can be in.
/// The state determines how we react to new transcription words.
enum AgentState: CustomStringConvertible {
    /// Listening for user speech. Silence timer runs when words are detected.
    case listening
    /// The `say` process is generating the TTS audio file (~1s).
    /// User 2+ words here â†’ kill process, cache response, back to listening.
    case generatingTTS
    /// AVAudioPlayerNode is playing TTS audio through speakers.
    /// User 2+ words here â†’ stop playback, back to listening.
    case playingTTS
    
    var description: String {
        switch self {
        case .listening: return "LISTENING"
        case .generatingTTS: return "GENERATING"
        case .playingTTS: return "PLAYING"
        }
    }
}

// MARK: - Conversation Turn Record

/// Record of a single conversation turn for the thread log.
/// We keep ALL turns, including interrupted ones, per instructions.mdc section 3:
/// "Never drop user speech from the thread, even if it happened mid-pipeline."
/// "Keep the full agent response in the conversation thread (do not trim it)."
struct ConversationTurn {
    let turnNumber: Int
    let userText: String
    let agentResponse: String
    let wasInterruptedDuringGeneration: Bool
    let wasInterruptedDuringPlayback: Bool
    let usedCache: Bool
}

// MARK: - Main Test

@available(macOS 26.0, *)
class TurnDetectionTest {
    
    // â”€â”€ Audio engine (from confirmed-working VPIO test) â”€â”€
    var audioEngine = AVAudioEngine()
    let playerNode = AVAudioPlayerNode()
    
    // â”€â”€ SpeechAnalyzer â”€â”€
    var analyzer: SpeechAnalyzer?
    var transcriber: SpeechTranscriber?
    var continuation: AsyncStream<AnalyzerInput>.Continuation?
    var resultTask: Task<Void, Error>?
    var frameCount: Int64 = 0
    
    // â”€â”€ State machine â”€â”€
    var state: AgentState = .listening
    let testStart = Date()
    
    // â”€â”€ Turn tracking â”€â”€
    /// Completed turn count. We run until this reaches maxTurns.
    /// A "completed turn" = TTS finished playing OR was interrupted during playback.
    /// Interrupted-during-generation does NOT count as a completed turn (response was cached, not spoken).
    var completedTurns = 0
    let maxTurns = 5
    
    /// The full text of what the user said in the current turn so far.
    /// This is the NEW text since the last turn boundary, extracted by comparing
    /// the total transcription against `textLengthAtTurnStart`.
    var currentTurnText = ""
    
    /// The entire raw transcription from SpeechAnalyzer for the full session.
    /// SpeechAnalyzer gives progressive/volatile results. We always store the
    /// latest (longest) version. When it revises, we take the revision.
    var fullSessionTranscription = ""
    
    /// Character count of `fullSessionTranscription` at the start of the current turn.
    /// Everything after this point belongs to the current turn.
    var textLengthAtTurnStart = 0
    
    /// Word count of the latest transcription result. Used to detect new words.
    /// We compare successive results: if word count goes up, new words appeared.
    var lastResultWordCount = 0
    
    /// Timestamp of the most recent new-word detection. The silence timer measures from here.
    var lastNewWordTime = Date()
    
    // â”€â”€ Silence timer â”€â”€
    var silenceTimerTask: Task<Void, Never>?
    let silenceThreshold: Double = 2.0
    
    // â”€â”€ Response cache â”€â”€
    /// The cached response text. Set when user interrupts during TTS generation.
    /// Cleared when: (a) cache is used, (b) cache expires, (c) TTS plays successfully.
    var cachedResponse: String? = nil
    
    /// When the cache timer started. Per instructions.mdc section 2:
    /// "start a 5-second cache timer from the moment the user resumes speaking"
    var cacheTimerStart: Date? = nil
    
    let cacheExpiry: Double = 5.0
    
    // â”€â”€ TTS control â”€â”€
    /// Handle to the running `say` process so we can kill it on interruption.
    var sayProcess: Process? = nil
    
    /// Flag: set true when we request cancellation of TTS generation.
    /// The generateAndPlay function checks this after waitUntilExit to know
    /// whether it was interrupted vs finished normally.
    var generationWasCancelled = false
    
    // â”€â”€ Interruption detection â”€â”€
    /// Word count snapshot taken when agent enters generatingTTS or playingTTS.
    /// Interruption = (currentWordCount - this) >= 2.
    var wordCountAtAgentStart = 0
    
    // â”€â”€ SpeechAnalyzer diagnostics â”€â”€
    // These track the health and behavior of SpeechAnalyzer to understand WHY
    // it has long gaps (3-57 seconds) between result batches even with continuous audio.
    //
    // HYPOTHESIS: VPIO zeros the mic input during TTS playback (echo cancellation).
    // This feeds near-zero audio to SpeechAnalyzer, which may confuse its internal
    // voice activity detector (VAD). After TTS stops, the VAD needs time to recalibrate,
    // causing the long gap before results resume.
    //
    // ALTERNATE HYPOTHESIS: SpeechAnalyzer accumulates internally and only delivers
    // when it has a confident sentence-level transcription, regardless of input quality.
    
    /// Total buffers yielded to SpeechAnalyzer's AsyncStream since the test started.
    var totalBuffersYielded: Int = 0
    
    /// Buffers yielded since the last SpeechAnalyzer result. Resets on each new result.
    /// If this is high (>200) when a result arrives, it means SpeechAnalyzer was processing
    /// for a long time before emitting anything.
    var buffersSinceLastResult: Int = 0
    
    /// Current RMS energy of the mic input, updated every tap callback.
    /// Used ONLY for diagnostics (not for silence detection â€” the user correctly noted
    /// that RMS picks up non-speech noise like music, ambient sound, etc.).
    var currentRMSEnergy: Float = 0.0
    
    /// Timestamp of last diagnostic log for periodic reporting.
    var lastDiagLogTime = Date.distantPast
    
    /// Timestamp of last SpeechAnalyzer result delivery, for measuring gaps.
    var lastResultDeliveryTime = Date.distantPast
    
    // â”€â”€ Conversation thread â”€â”€
    var conversationThread: [ConversationTurn] = []
    
    // â”€â”€ Completion signal â”€â”€
    var done: CheckedContinuation<Void, Never>?
    
    // MARK: - Logging
    
    func log(_ msg: String) {
        let t = String(format: "%6.1f", Date().timeIntervalSince(testStart))
        print("[T+\(t)s] [\(state)] \(msg)")
        fflush(stdout)
    }
    
    func logState(_ from: AgentState, _ to: AgentState) {
        let t = String(format: "%6.1f", Date().timeIntervalSince(testStart))
        print("[T+\(t)s] STATE: \(from) â†’ \(to)")
        fflush(stdout)
    }
    
    // MARK: - Run
    
    func run() async throws {
        print("================================================================")
        print("  FULL Turn Detection + Caching + Interruption Test")
        print("  VPIO Echo Cancellation | \(maxTurns) completed turns")
        print("  Play audio from your phone as the 'user'")
        print("================================================================")
        print("")
        print("  Paths tested:")
        print("    A) Normal:    silence â†’ generate â†’ play â†’ finish")
        print("    B) Cache hit: silence â†’ generate â†’ interrupt â†’ cache â†’ silence â†’ speak cache")
        print("    C) Cache exp: silence â†’ generate â†’ interrupt â†’ cache â†’ >5s â†’ discard â†’ fresh")
        print("    D) Play int:  playing â†’ user speaks â†’ stop playback")
        print("")
        
        // Initialize log file
        initializeLogFile()
        
        // â”€â”€ Setup audio engine with VPIO (from confirmed-working test) â”€â”€
        try audioEngine.outputNode.setVoiceProcessingEnabled(true)
        let outFmt = audioEngine.outputNode.outputFormat(forBus: 0)
        audioEngine.connect(audioEngine.mainMixerNode, to: audioEngine.outputNode, format: outFmt)
        audioEngine.attach(playerNode)
        audioEngine.connect(playerNode, to: audioEngine.mainMixerNode, format: nil)
        
        // â”€â”€ Setup SpeechAnalyzer (from confirmed-working test) â”€â”€
        guard let locale = await SpeechTranscriber.supportedLocale(
            equivalentTo: Locale(identifier: "en-US")) else {
            log("FATAL: en-US not supported"); return
        }
        // CRITICAL: Must use BOTH .volatileResults AND .fastResults together.
        // .volatileResults alone: SpeechAnalyzer batches results in ~3.8 second chunks.
        // .volatileResults + .fastResults: Streams word-by-word at 200-500ms intervals.
        // This was discovered by analyzing the SwiftUI_SpeechAnalyzerDemo repo which uses
        // the .timeIndexedProgressiveTranscription preset (maps to these exact flags).
        // Without .fastResults, the 2-second silence timer ALWAYS false-triggers because
        // the 3.8s batch gap is longer than the 2.0s threshold.
        // Also adding .audioTimeRange for precise speech timing per result.
        let xscriber = SpeechTranscriber(
            locale: locale, transcriptionOptions: [],
            reportingOptions: [.volatileResults, .fastResults],
            attributeOptions: [.audioTimeRange])
        self.transcriber = xscriber
        guard let aFmt = await SpeechAnalyzer.bestAvailableAudioFormat(
            compatibleWith: [xscriber]) else {
            log("FATAL: no audio format"); return
        }
        let xanalyzer = SpeechAnalyzer(modules: [xscriber])
        self.analyzer = xanalyzer
        let (stream, cont) = AsyncStream<AnalyzerInput>.makeStream()
        self.continuation = cont
        
        // â”€â”€ Result consumer task â”€â”€
        resultTask = Task { [weak self] in
            guard let self else { return }
            for try await result in xscriber.results {
                let raw = String(result.text.characters)
                let text = raw.trimmingCharacters(in: .whitespaces)
                guard !text.isEmpty else { continue }
                self.onTranscriptionResult(text)
            }
        }
        
        // â”€â”€ Audio tap â†’ SpeechAnalyzer (from confirmed-working test) â”€â”€
        let inNode = audioEngine.inputNode
        let inFmt = inNode.outputFormat(forBus: 0)
        let tapFmt = AVAudioFormat(commonFormat: .pcmFormatFloat32,
                                    sampleRate: inFmt.sampleRate, channels: 1, interleaved: false)!
        guard let conv = AVAudioConverter(from: tapFmt, to: aFmt) else {
            log("FATAL: no converter"); return
        }
        inNode.installTap(onBus: 0, bufferSize: 1024, format: tapFmt) { [weak self] buf, _ in
            guard let self else { return }
            
            // â”€â”€ Calculate RMS energy for diagnostics only â”€â”€
            if let data = buf.floatChannelData?[0] {
                var sumSquares: Float = 0
                let count = Int(buf.frameLength)
                for i in 0..<count { sumSquares += data[i] * data[i] }
                self.currentRMSEnergy = sqrt(sumSquares / Float(max(count, 1)))
            }
            
            // â”€â”€ Periodic diagnostic log (every 2s to reduce noise) â”€â”€
            // Shows: RMS energy of mic input, buffers fed since last SpeechAnalyzer result,
            // and time since last result delivery. This tells us whether SpeechAnalyzer is
            // getting audio and how long it's been "thinking" without delivering.
            let now = Date()
            if now.timeIntervalSince(self.lastDiagLogTime) >= 2.0 {
                self.lastDiagLogTime = now
                let rms = String(format: "%.4f", self.currentRMSEnergy)
                let t = String(format: "%6.1f", now.timeIntervalSince(self.testStart))
                let sinceResult = self.lastResultDeliveryTime == Date.distantPast
                    ? "never"
                    : String(format: "%.1fs ago", now.timeIntervalSince(self.lastResultDeliveryTime))
                let sinceWord = String(format: "%.1fs", now.timeIntervalSince(self.lastNewWordTime))
                print("[T+\(t)s] [\(self.state)] DIAG rms=\(rms) bufs_since_result=\(self.buffersSinceLastResult) total_bufs=\(self.totalBuffersYielded) last_result=\(sinceResult) last_word=\(sinceWord)")
                fflush(stdout)
            }
            
            // â”€â”€ Feed to SpeechAnalyzer â”€â”€
            let ratio = aFmt.sampleRate / tapFmt.sampleRate
            let cap = AVAudioFrameCount(Double(buf.frameLength) * ratio)
            guard cap > 0, let out = AVAudioPCMBuffer(pcmFormat: aFmt, frameCapacity: cap) else { return }
            var err: NSError?
            conv.convert(to: out, error: &err) { _, s in s.pointee = .haveData; return buf }
            if err == nil && out.frameLength > 0 {
                let t = CMTime(value: CMTimeValue(self.frameCount), timescale: CMTimeScale(aFmt.sampleRate))
                self.frameCount += Int64(out.frameLength)
                self.continuation?.yield(AnalyzerInput(buffer: out, bufferStartTime: t))
                self.totalBuffersYielded += 1
                self.buffersSinceLastResult += 1
            }
        }
        
        // â”€â”€ Start â”€â”€
        audioEngine.prepare()
        try audioEngine.start()
        try await xanalyzer.start(inputSequence: stream)
        log("Engine + SpeechAnalyzer started. Listening...")
        print("")
        
        // â”€â”€ Wait for completion or 3-minute timeout â”€â”€
        await withCheckedContinuation { (c: CheckedContinuation<Void, Never>) in
            self.done = c
            Task { try? await Task.sleep(for: .seconds(180)); self.finish("TIMEOUT 3min") }
        }
        
        // â”€â”€ Cleanup â”€â”€
        playerNode.stop()
        audioEngine.stop()
        inNode.removeTap(onBus: 0)
        continuation?.finish()
        resultTask?.cancel()
        
        // â”€â”€ Print conversation thread â”€â”€
        print("")
        print("================================================================")
        print("  CONVERSATION THREAD (\(conversationThread.count) turns)")
        print("================================================================")
        for turn in conversationThread {
            let flags = [
                turn.wasInterruptedDuringGeneration ? "INT-GEN/CACHED" : nil,
                turn.wasInterruptedDuringPlayback ? "INT-PLAY" : nil,
                turn.usedCache ? "FROM-CACHE" : nil
            ].compactMap { $0 }.joined(separator: ", ")
            let flagStr = flags.isEmpty ? "" : " [\(flags)]"
            print("  USER \(turn.turnNumber): \"\(turn.userText)\"")
            print("  AI   \(turn.turnNumber): \"\(turn.agentResponse)\"\(flagStr)")
            print("")
        }
        print("================================================================")
        print("  TEST FINISHED â€” \(completedTurns) completed turns")
        print("================================================================")
    }
    
    func finish(_ reason: String) {
        log("FINISH: \(reason)")
        done?.resume()
        done = nil
    }
    
    // MARK: - Transcription Result Handler
    
    /// Tracks the last time we logged a WORDS line, to debounce burst logging.
    /// SpeechAnalyzer delivers 5-15 progressive results at the exact same timestamp.
    /// Logging each one creates noise. Instead we log the first and set a debounce
    /// timer to log the final state of the burst 100ms later.
    var wordLogDebounceTask: Task<Void, Never>?
    
    /// Called for every progressive transcription result from SpeechAnalyzer.
    /// This is the central dispatch: it detects new words, manages the silence timer,
    /// and triggers interruptions based on the current agent state.
    func onTranscriptionResult(_ text: String) {
        // â”€â”€ Detect new words â”€â”€
        let words = text.split(separator: " ", omittingEmptySubsequences: true)
        let wc = words.count
        let isNew = wc > lastResultWordCount
        let delta = wc - lastResultWordCount
        
        // Update the full session transcription and extract current turn text.
        // SpeechAnalyzer gives progressive results that may revise earlier text,
        // so we always take the latest version and slice from turnStart.
        fullSessionTranscription = text
        if text.count > textLengthAtTurnStart {
            let startIdx = text.index(text.startIndex,
                offsetBy: min(textLengthAtTurnStart, text.count))
            currentTurnText = String(text[startIdx...]).trimmingCharacters(in: .whitespaces)
        }
        
        guard isNew else { return } // No new words â€” nothing to do.
        
        // â”€â”€ Diagnostic: how long did SpeechAnalyzer take between result batches? â”€â”€
        let now = Date()
        let gapSinceResult = lastResultDeliveryTime == Date.distantPast
            ? -1.0
            : now.timeIntervalSince(lastResultDeliveryTime)
        if gapSinceResult > 3.0 {
            // Log when SpeechAnalyzer had a long gap (>3s) â€” these are the problematic pauses.
            let msg = "âš ï¸ ANALYZER GAP: \(String(format: "%.1f", gapSinceResult))s since last result, \(buffersSinceLastResult) buffers fed during gap, rms_now=\(String(format: "%.4f", currentRMSEnergy))"
            log(msg)
            writeToLog(msg)
        }
        lastResultDeliveryTime = now
        buffersSinceLastResult = 0
        
        lastResultWordCount = wc
        lastNewWordTime = now
        
        // â”€â”€ Route based on state â”€â”€
        switch state {
            
        case .listening:
            // Debounced logging: SpeechAnalyzer delivers 5-15 progressive results in
            // the same millisecond. We only log the FINAL state of each burst to keep
            // output readable. Cancel previous debounce and start a new 150ms timer.
            wordLogDebounceTask?.cancel()
            wordLogDebounceTask = Task { [weak self, currentText = currentTurnText, d = delta, fullText = text] in
                try? await Task.sleep(for: .milliseconds(150))
                guard !Task.isCancelled, let self else { return }
                let t = String(format: "%6.1f", Date().timeIntervalSince(self.testStart))
                self.log("WORDS (+\(d) in burst): \"\(currentText)\"")
                // Log to file with full details
                writeToLog("[T+\(t)s] [LISTENING] RESULT: wordCount=\(self.lastResultWordCount), delta=+\(d), text=\"\(fullText)\"")
            }
            restartSilenceTimer()
            
        case .generatingTTS:
            let sinceTurn = wc - wordCountAtAgentStart
            if sinceTurn >= 2 {
                // â”€â”€ INTERRUPTION DURING GENERATION â”€â”€
                // Per instructions.mdc Â§4: kill say, cache response, back to listening.
                // Per Â§2: cache timer starts NOW (when user resumed speaking).
                log(">>> INTERRUPT DURING GENERATION (\(sinceTurn) words since agent start)")
                log("    User said: \"\(currentTurnText)\"")
                
                generationWasCancelled = true
                if let p = sayProcess, p.isRunning {
                    p.terminate()
                    log("    Killed say process (PID \(p.processIdentifier))")
                }
                // Cache is already set by generateAndPlayTTS before it started.
                // Set the cache timer to NOW.
                cacheTimerStart = Date()
                log("    Response cached. Cache timer started (\(cacheExpiry)s window).")
                
                let oldState = state
                state = .listening
                logState(oldState, state)
                
                // Start tracking this as a new turn segment.
                startNewTurnSegment()
                restartSilenceTimer()
            } else {
                log("SPEECH (\(sinceTurn) word) during generation: \"\(currentTurnText)\"")
            }
            
        case .playingTTS:
            let sinceTurn = wc - wordCountAtAgentStart
            if sinceTurn >= 2 {
                // â”€â”€ INTERRUPTION DURING PLAYBACK â”€â”€
                // Per instructions.mdc Â§4: immediately stop playback, end agent turn.
                log(">>> INTERRUPT DURING PLAYBACK (\(sinceTurn) words since agent start)")
                log("    User said: \"\(currentTurnText)\"")
                
                playerNode.stop()
                log("    Stopped AVAudioPlayerNode")
                
                let oldState = state
                state = .listening
                logState(oldState, state)
                
                startNewTurnSegment()
                restartSilenceTimer()
            } else {
                log("SPEECH (\(sinceTurn) word) during playback: \"\(currentTurnText)\"")
            }
        }
    }
    
    // MARK: - Silence Timer
    
    /// Restarts the 2-second silence timer. Every new word calls this.
    /// When 2 seconds pass without cancellation, `onSilenceTimerFired` is called.
    ///
    /// NOTE: We previously tried an audio energy gate here to wait for RMS to drop,
    /// but the user correctly identified that RMS picks up non-speech sounds (music,
    /// ambient noise) and would prevent the timer from ever firing in noisy environments.
    ///
    /// RESOLVED (2026-02-09): The "bursty delivery" with 3-57 second gaps was caused by
    /// missing `.fastResults` in SpeechTranscriber's reportingOptions. With `.fastResults`
    /// enabled, results stream at 200-500ms intervals and the 2-second silence timer
    /// works reliably. See 1-priority-documents/SpeechAnalyzer-Definitive-Configuration.md.
    func restartSilenceTimer() {
        silenceTimerTask?.cancel()
        silenceTimerTask = Task { [weak self] in
            try? await Task.sleep(for: .seconds(self?.silenceThreshold ?? 2.0))
            guard !Task.isCancelled, let self, self.state == .listening else { return }
            self.onSilenceTimerFired()
        }
    }
    
    // MARK: - Turn End
    
    /// Called when 2 seconds of silence are detected.
    /// Decides whether to use a cached response or generate a fresh one.
    func onSilenceTimerFired() {
        let turnText = currentTurnText.trimmingCharacters(in: .whitespaces)
        
        guard !turnText.isEmpty else {
            log("SILENCE 2s but empty transcription â€” ignoring")
            return
        }
        
        log("â”€â”€â”€â”€ SILENCE 2.0s DETECTED â”€â”€â”€â”€")
        log("  Turn text: \"\(turnText)\"")
        
        // â”€â”€ Check cache â”€â”€
        if let cached = cachedResponse, let cStart = cacheTimerStart {
            let age = Date().timeIntervalSince(cStart)
            if age < cacheExpiry {
                // â”€â”€ PATH B: Cache hit â”€â”€
                log("  CACHE HIT â€” age \(String(format: "%.1f", age))s < \(cacheExpiry)s")
                log("  Speaking cached response: \"\(cached)\"")
                
                // Record this turn (the speech during cache window)
                conversationThread.append(ConversationTurn(
                    turnNumber: conversationThread.count + 1,
                    userText: turnText,
                    agentResponse: cached,
                    wasInterruptedDuringGeneration: false,
                    wasInterruptedDuringPlayback: false,
                    usedCache: true
                ))
                
                let response = cached
                cachedResponse = nil
                cacheTimerStart = nil
                
                Task { await self.generateAndPlayTTS(response: response, isCached: true) }
                return
            } else {
                // â”€â”€ PATH C: Cache expired â”€â”€
                log("  CACHE EXPIRED â€” age \(String(format: "%.1f", age))s > \(cacheExpiry)s")
                log("  Discarding cached response. Will generate fresh.")
                cachedResponse = nil
                cacheTimerStart = nil
                // Fall through to generate fresh response
            }
        }
        
        // â”€â”€ PATH A / fresh after PATH C: Generate new response â”€â”€
        completedTurns += 1
        let turnNum = completedTurns
        
        if turnNum > maxTurns {
            finish("ALL \(maxTurns) TURNS COMPLETE")
            return
        }
        
        let response = "Turn \(turnNum). \(turnText)"
        log("  NEW RESPONSE for turn \(turnNum)/\(maxTurns): \"\(response)\"")
        
        // Record in conversation thread
        conversationThread.append(ConversationTurn(
            turnNumber: turnNum,
            userText: turnText,
            agentResponse: response,
            wasInterruptedDuringGeneration: false,
            wasInterruptedDuringPlayback: false,
            usedCache: false
        ))
        
        Task { await self.generateAndPlayTTS(response: response, isCached: false) }
    }
    
    // MARK: - TTS Pipeline
    
    /// Full TTS pipeline: generate audio file with `say`, then play through AVAudioEngine.
    ///
    /// At any point, onTranscriptionResult may interrupt us by:
    ///   - During generation: setting generationWasCancelled=true and killing the process
    ///   - During playback: calling playerNode.stop() and setting state to .listening
    ///
    /// If interrupted during generation, the response was already set as cachedResponse
    /// before we started. The cache timer is set by onTranscriptionResult.
    ///
    /// If interrupted during playback, we mark the turn as interrupted in the thread.
    func generateAndPlayTTS(response: String, isCached: Bool) async {
        // â”€â”€ PHASE 1: Generate TTS file â”€â”€
        generationWasCancelled = false
        wordCountAtAgentStart = lastResultWordCount
        
        // Pre-set cache so if interrupted, the response is already saved.
        // Only set cache for fresh responses. If this IS a cached response being spoken,
        // we don't re-cache it (it's already been cached once).
        if !isCached {
            cachedResponse = response
        } else {
            cachedResponse = nil
        }
        cacheTimerStart = nil // only set when user actually speaks
        
        let oldState = state
        state = .generatingTTS
        logState(oldState, .generatingTTS)
        
        let path = "/tmp/turn_tts_\(completedTurns)_\(Int(Date().timeIntervalSince1970)).aiff"
        let url = URL(fileURLWithPath: path)
        try? FileManager.default.removeItem(at: url)
        
        let genStart = Date()
        log("GENERATING: \"\(response)\"")
        
        let proc = Process()
        proc.executableURL = URL(fileURLWithPath: "/usr/bin/say")
        proc.arguments = ["-o", path, response]
        sayProcess = proc
        
        do { try proc.run() } catch {
            log("ERROR: say failed: \(error)")
            state = .listening
            return
        }
        
        // This blocks until say finishes or is killed by the interrupt handler.
        proc.waitUntilExit()
        sayProcess = nil
        
        let genMs = Int(Date().timeIntervalSince(genStart) * 1000)
        
        // â”€â”€ Check if we were interrupted during generation â”€â”€
        if generationWasCancelled || state != .generatingTTS {
            log("GENERATION INTERRUPTED after \(genMs)ms")
            // The interrupt handler already:
            //   - transitioned state to .listening
            //   - set cacheTimerStart
            //   - started silence timer
            //   - started new turn segment
            // We just need to record this interrupted turn.
            if !isCached, let lastTurn = conversationThread.last {
                // Update the last turn record to mark it as interrupted during gen
                conversationThread[conversationThread.count - 1] = ConversationTurn(
                    turnNumber: lastTurn.turnNumber,
                    userText: lastTurn.userText,
                    agentResponse: lastTurn.agentResponse,
                    wasInterruptedDuringGeneration: true,
                    wasInterruptedDuringPlayback: false,
                    usedCache: lastTurn.usedCache
                )
            }
            // completedTurns is NOT incremented â€” generation-interrupted turns don't count.
            // The response is cached and might be spoken on the next silence.
            if !isCached {
                // We already incremented completedTurns in onSilenceTimerFired, undo it
                completedTurns -= 1
            }
            try? FileManager.default.removeItem(at: url)
            return
        }
        
        // â”€â”€ Generation succeeded, check file â”€â”€
        guard FileManager.default.fileExists(atPath: path),
              let file = try? AVAudioFile(forReading: url) else {
            log("ERROR: TTS file missing or unreadable")
            state = .listening
            return
        }
        
        let dur = Double(file.length) / file.fileFormat.sampleRate
        log("GENERATED in \(genMs)ms, duration \(String(format: "%.1f", dur))s")
        
        // â”€â”€ PHASE 2: Play the TTS audio â”€â”€
        // Clear cache since we're actually playing now.
        cachedResponse = nil
        cacheTimerStart = nil
        
        // Reset the interruption word counter for the playback phase.
        wordCountAtAgentStart = lastResultWordCount
        
        let playOld = state
        state = .playingTTS
        logState(playOld, .playingTTS)
        log("PLAYING TTS (\(String(format: "%.1f", dur))s)...")
        
        // Play and wait for completion (or interruption).
        let playbackFinished = await withCheckedContinuation { (c: CheckedContinuation<Bool, Never>) in
            var resumed = false
            playerNode.scheduleFile(file, at: nil) {
                // This fires when the file finishes playing OR when .stop() is called.
                if !resumed {
                    resumed = true
                    c.resume(returning: true)
                }
            }
            playerNode.play()
        }
        
        try? FileManager.default.removeItem(at: url)
        
        // â”€â”€ Check if interrupted during playback â”€â”€
        if state != .playingTTS {
            // Interrupted by onTranscriptionResult which already set state to .listening
            log("PLAYBACK INTERRUPTED")
            
            // Mark in conversation thread
            if let lastTurn = conversationThread.last {
                conversationThread[conversationThread.count - 1] = ConversationTurn(
                    turnNumber: lastTurn.turnNumber,
                    userText: lastTurn.userText,
                    agentResponse: lastTurn.agentResponse,
                    wasInterruptedDuringGeneration: lastTurn.wasInterruptedDuringGeneration,
                    wasInterruptedDuringPlayback: true,
                    usedCache: lastTurn.usedCache
                )
            }
            // Playback interruption DOES count as a completed turn
            // (the agent spoke, even if cut short).
            
            if completedTurns >= maxTurns {
                finish("ALL \(maxTurns) TURNS COMPLETE")
            }
            return
        }
        
        // â”€â”€ Playback finished normally â”€â”€
        log("PLAYBACK FINISHED normally")
        let finOld = state
        state = .listening
        logState(finOld, .listening)
        
        startNewTurnSegment()
        
        if completedTurns >= maxTurns {
            finish("ALL \(maxTurns) TURNS COMPLETE")
        }
    }
    
    // MARK: - Turn Segmentation
    
    /// Marks the current position in the transcription stream as the start of a new turn.
    /// Everything transcribed after this point belongs to the next turn.
    func startNewTurnSegment() {
        textLengthAtTurnStart = fullSessionTranscription.count
        currentTurnText = ""
        // Do NOT reset lastResultWordCount â€” it's cumulative for the session.
        // Do NOT reset wordCountAtAgentStart â€” it's set fresh when agent starts.
    }
}

// MARK: - Entry Point

@main
struct TurnDetectionEntry {
    static func main() {
        setStdoutUnbuffered()
        guard #available(macOS 26.0, *) else { print("Requires macOS 26+"); exit(1) }
        Task {
            let test = TurnDetectionTest()
            do { try await test.run() }
            catch { print("FATAL: \(error)") }
            closeLogFile()
            exit(0)
        }
        RunLoop.main.run()
    }
}
